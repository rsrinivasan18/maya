# MAYA Development Log - 2026-02-27

**Date:** Friday, 27 February 2026
**Developer:** Srinivasan
**Session Goal:** Add Speech-to-Text using faster-whisper
**Status:** Completed

---

## Session Summary

MAYA can now hear. Added a complete STT layer using faster-whisper running
fully offline on the laptop. Srinika can speak in Hindi, English, or Hinglish
and MAYA transcribes it before passing to the LangGraph pipeline.

Decision: Using faster-whisper (not openai-whisper) because it's 4x faster
on CPU via CTranslate2 int8 quantization - critical for a responsive kids AI.

---

## What Was Built

### New Files
| File | Purpose |
|------|---------|
| `src/maya/stt/__init__.py` | STT package |
| `src/maya/stt/transcriber.py` | `STTEngine` class - record + transcribe |
| `test_stt.py` | Standalone mic test (run this first!) |

### Modified Files
| File | Change |
|------|--------|
| `requirements.txt` | Added faster-whisper, sounddevice, numpy |
| `chat_loop.py` | Added `--voice`, `--record-time` flags + voice input loop |

---

## STTEngine Architecture

```
Microphone
    ↓
sounddevice.rec()          Records N seconds of float32 audio at 16kHz
    ↓
numpy array (N,)           1D audio data
    ↓
WhisperModel.transcribe()  faster-whisper with vad_filter=True
    ↓
text string                "Namaste MAYA, kya hal hai?"
    ↓
maya_graph.invoke()        Existing LangGraph pipeline (unchanged!)
```

---

## Key Technical Decisions

| Decision | Choice | Reason |
|----------|--------|--------|
| STT library | faster-whisper | 4x faster than openai-whisper on CPU |
| Audio library | sounddevice | Bundles PortAudio on Windows, no extra install |
| Model size | base | Good balance speed/accuracy (~145MB) |
| Language | None (auto-detect) | Handles Hinglish naturally |
| VAD | vad_filter=True | Built-in silence detection, no garbage transcription |
| compute_type | int8 | Fastest on CPU, same accuracy as float32 |

---

## How to Run

```bash
.venv\Scripts\activate

# Step 1: Install new dependencies
pip install -r requirements.txt

# Step 2: Test microphone (first time setup)
python test_stt.py

# Step 3: Voice conversation with MAYA!
python chat_loop.py --voice

# With longer recording window (for slower speakers)
python chat_loop.py --voice --record-time 7

# With debug trace visible
python chat_loop.py --voice --debug

# Keyboard mode still works exactly as before
python chat_loop.py
```

---

## What Whisper Can Handle (for Srinika)

| Language | Example | Expected Result |
|----------|---------|-----------------|
| English | "What is photosynthesis?" | ✅ Excellent |
| Hindi | "Namaste, kya hal hai?" | ✅ Good with base |
| Hinglish | "Hello MAYA, mujhe gravity ke baare mein batao" | ✅ Good |
| Numbers | "5 plus 3 kya hoga?" | ✅ Good |

If Hindi accuracy is low → switch to `small` model in `transcriber.py` (465MB).

---

## The LangGraph Pipeline is Untouched

This is the key architectural win. The STT layer sits OUTSIDE the graph:

```
Voice → [STTEngine.listen()] → text string → [maya_graph.invoke()] → response
```

The graph nodes (detect_language, understand_intent, response nodes) are
completely unchanged. STT is just a new input source for the same text pipeline.
This is how good architecture works - adding new capabilities without breaking
what already works.

---

## Next Steps

### Immediate
- [ ] Run `pip install -r requirements.txt` to install faster-whisper
- [ ] Run `python test_stt.py` - verify mic and transcription
- [ ] Run `python chat_loop.py --voice` - have first voice conversation
- [ ] Srinika says "Namaste MAYA!" and MAYA responds in Hindi!

### Next Session (Week 3)
- [ ] Ollama LLM integration - replace rule-based help_response with real AI
- [ ] MAYA gives real answers to "What is photosynthesis?" etc.

### Week 2 Remaining
- [ ] Piper TTS - MAYA speaks back (text → voice)
- [ ] Full voice loop: speak → transcribe → graph → TTS → speak

---

## Notes

> The architecture decision to keep STT outside the graph is deliberate.
> In future, the graph can have a "transcription" node if needed,
> but for now keeping it as a pre-processing step is cleaner.
>
> faster-whisper's VAD filter is doing heavy lifting here -
> it automatically ignores silence and background noise.
> Without it, Whisper would transcribe silence as random text.
